
1950s-1960s:
- "Computing Machinery and Intelligence" (Turing, 1950) - [paper](https://www.cs.princeton.edu/~chazelle/courses/BIB/turing-intelligence.pdf)
  - Introduced the Turing Test and fundamental questions about machine intelligence
  - First serious discussion of whether machines can think

- "A Logical Calculus of Ideas Immanent in Nervous Activity" (McCulloch & Pitts, 1943) - [paper](https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf)
  - Laid groundwork for neural networks and computational theory of mind
  - Showed how simple neural networks could compute logical functions

1970s-1980s:
- "A Framework for Representing Knowledge" (Minsky, 1974) - [paper](https://courses.media.mit.edu/2004spring/mas966/Minsky%201974%20Framework%20for%20knowledge.pdf)
  - Introduced frames as a way to represent knowledge
  - Influenced modern knowledge representation systems

- "Learning representations by back-propagating errors" (Rumelhart, Hinton & Williams, 1986) - [paper](https://www.cs.utoronto.ca/~hinton/absps/naturebp.pdf)
  - Popularized backpropagation for training neural networks
  - Enabled practical training of deep neural networks

1990s-2000s:
- "Long Short-Term Memory" (Hochreiter & Schmidhuber, 1997) - [paper](https://deeplearning.cs.cmu.edu/S23/document/readings/LSTM.pdf)
  - Introduced LSTM networks
  - Solved the vanishing gradient problem for recurrent neural networks

- "Gradient-Based Learning Applied to Document Recognition" (LeCun et al., 1998) - [paper](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)
  - Introduced ConvNets and the LeNet architecture
  - Pioneered deep learning for computer vision

2010s:
- "ImageNet Classification with Deep Convolutional Neural Networks" (Krizhevsky et al., 2012) - [paper](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
  - AlexNet paper that sparked the deep learning revolution
  - Demonstrated the power of deep CNNs trained on large datasets

- "Attention Is All You Need" (Vaswani et al., 2017) - [paper](https://arxiv.org/pdf/1706.03762)
  - Introduced the Transformer architecture
  - Foundation for modern language models like GPT and BERT

2018-2020:

- "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" (Devlin et al., 2018) - [paper](https://arxiv.org/pdf/1810.04805)
  - Introduced bidirectional context for language understanding
  - Set new standards for NLP tasks

- "Language Models are Few-Shot Learners" (Brown et al., 2020) - GPT-3 paper - [paper](https://arxiv.org/pdf/2005.14165)
  - Demonstrated emergent abilities in large language models
  - Showed scaling effects on model capabilities
  - Introduced few-shot learning through prompting

- "High-Resolution Image Synthesis with Latent Diffusion Models" (Rombach et al., 2022) - Stable Diffusion paper - [paper](https://arxiv.org/pdf/2112.10752)
  - Made efficient image generation possible on consumer hardware
  - Advanced the field of text-to-image generation

2021-2023:

- "Constitutional AI: Harmlessness from AI Feedback" (Yuntao et al., 2022) - [paper](https://arxiv.org/pdf/2212.08073)
  - Introduced methods for aligning AI systems with human values
  - Proposed frameworks for AI safety

- "PaLM: Scaling Language Modeling with Pathways" (Chowdhery et al., 2022) - [paper](https://arxiv.org/pdf/2204.02311)
  - Advanced scaling laws for language models
  - Demonstrated breakthrough capabilities in reasoning and code generation

- "Training language models to follow instructions with human feedback" (Long et al., 2022) - InstructGPT paper - [paper](https://arxiv.org/pdf/2203.02155)
  - Pioneered instruction tuning using human feedback (RLHF)
  - Improved model alignment with human intent

- "LLaMA: Open and Efficient Foundation Language Models" (Touvron et al., 2023) - [paper](https://arxiv.org/pdf/2302.13971)
  - Demonstrated efficient training of powerful open-source language models
  - Sparked widespread development of open LLMs
